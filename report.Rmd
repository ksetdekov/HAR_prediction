---
title: "Practical machine learning  lectures 4"
author: "Kirill Setdekov"
date: "15 01 2020"
output:
  html_document:
    keep_md: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	cache = TRUE
)
```

## Task:
* The goal of your project is to predict the manner in which they did the exercise. 
* This is the "classe" variable in the training set. You may use any of the other variables to predict with. 
* You should create a report describing how you built your model, 
* how you used cross validation, 
* what you think the expected out of sample error is, and 
* why you made the choices you did. 
* You will also use your prediction model to predict 20 different test cases.


## Data loading

```{r dataload}
if(!file.exists("./data")) {
     dir.create("./data")
}
if (!file.exists("./data/training.csv") |
    !file.exists("./data/testing.csv")) {
     fileUrl1 <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
     fileUrl2 <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
     download.file(fileUrl1, destfile = "./data/training.csv")
     download.file(fileUrl2, destfile = "./data/testing.csv")
}

data <- read.csv("./data/training.csv")
quiz <- read.csv("./data/testing.csv")
```
## Train and test partition creation and NA handling

Some columns are full of NAs and are NA in the final 12 values, so I mark and remove them from the validation, and build dataset. Then i partition build dataset (original train csv) into train and test parts.

```{r traincontrol}
library(caret)
## temp
shortenbuild <- createDataPartition(y=data$classe, p=0.1, list = FALSE)
data <- data[shortenbuild,]

## temp

allmissing <- sapply(quiz, function(x)!all(is.na(x)))
data[is.na(data)] <- 0
quiz[is.na(quiz)] <- 0
data <- data[,allmissing]
quiz <- quiz[,allmissing]

data <- data[,-1] 
quiz <- quiz[,-1] 


inBuild <- createDataPartition(y = data$classe,
                               p = 0.7, list = FALSE)
validation <- data[-inBuild, ]
buildData <- data[inBuild, ]

inTrain <- createDataPartition(y = buildData$classe,
                               p = 0.7, list = FALSE)
training <- buildData[inTrain, ]
testing <- buildData[-inTrain, ]
dim(training)
dim(testing)
dim(validation)
```

### build 3 models 

I deliberately chose not to eliminate variables to test how different models perform on an unprocessed dataset.

I chose 4 methods: 
1. Generalized Boosted Model
2. Random forest
3. Linear discriminant analysis
4. Ensemble of the 1-3 above.

```{r modelling, warning=FALSE}
set.seed(45)
mod1 <- train(classe~., method ="gbm", data = training, verbose = T)
mod2 <- train(classe~., method ="rf", data = training, trControl = trainControl(method = "cv"),number = 3, verbose = T)
mod3 <- train(classe~., method ="lda", data = training, verbose = T)
```
### compare them
```{r dependson=c(-1)}
pred1 <- predict(mod1, testing)
pred2 <- predict(mod2, testing)
pred3 <- predict(mod3, testing)
predDF3 <- data.frame(pred1, pred2, pred3, classe = testing$classe)

MLmetrics::Accuracy(pred1,testing$classe)
MLmetrics::Accuracy(pred2,testing$classe)
MLmetrics::Accuracy(pred3,testing$classe)
```

### make a model that combines predictors

```{r dependson=c(-1, -2)}
combModFit3 <- train(classe~., method = "gbm", data = predDF3)
combPred <- predict(combModFit3,predDF3)
MLmetrics::Accuracy(combPred,testing$classe)
```

### on a validation
```{r}
pred1V <- predict(mod1, validation)
pred2V <- predict(mod2, validation)
pred3V <- predict(mod3, validation)

predVDF <-
    data.frame(
        pred1 = pred1V,
        pred2 = pred2V,
        pred3 = pred3V
    )
combPredV <- predict(combModFit3,predVDF)
MLmetrics::Accuracy(y_pred = pred1V,y_true = validation$classe)
MLmetrics::Accuracy(y_pred = pred2V,y_true = validation$classe)
MLmetrics::Accuracy(y_pred = pred3V,y_true = validation$classe)
## this is a combined model
MLmetrics::Accuracy(y_pred = combPredV,y_true = validation$classe)
confusionMatrix(combPredV,validation$classe)
```


